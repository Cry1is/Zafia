warning: LF will be replaced by CRLF in .ipynb_checkpoints/Notebook 2-checkpoint.ipynb.
The file will have its original line endings in your working directory
warning: LF will be replaced by CRLF in Notebook 2.ipynb.
The file will have its original line endings in your working directory
[1mdiff --git a/.ipynb_checkpoints/Notebook 2-checkpoint.ipynb b/.ipynb_checkpoints/Notebook 2-checkpoint.ipynb[m
[1mindex 22e6a55..64bea60 100644[m
[1m--- a/.ipynb_checkpoints/Notebook 2-checkpoint.ipynb[m	
[1m+++ b/.ipynb_checkpoints/Notebook 2-checkpoint.ipynb[m	
[36m@@ -46,11 +46,7 @@[m
      "name": "stdout",[m
      "output_type": "stream",[m
      "text": [[m
[31m-      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",[m
[31m-      "johnsnowlabs 4.4.4 requires pyspark==3.1.2, but you have pyspark 3.3.0 which is incompatible.\n",[m
[31m-      "johnsnowlabs 4.4.4 requires spark-nlp==4.4.1, but you have spark-nlp 4.3.2 which is incompatible.\n",[m
[31m-      "flair 0.11.1 requires more-itertools~=8.8.0, but you have more-itertools 8.12.0 which is incompatible.\u001b[0m\u001b[31m\n",[m
[31m-      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"[m
[32m+[m[32m      "Note: you may need to restart the kernel to use updated packages.\n"[m
      ][m
     }[m
    ],[m
[36m@@ -58,6 +54,15 @@[m
     "pip install -q pyspark==3.3.0 spark-nlp==4.3.2"[m
    ][m
   },[m
[32m+[m[32m  {[m
[32m+[m[32m   "cell_type": "code",[m
[32m+[m[32m   "execution_count": null,[m
[32m+[m[32m   "metadata": {},[m
[32m+[m[32m   "outputs": [],[m
[32m+[m[32m   "source": [[m
[32m+[m[32m    "pip uninstall spark-nlp"[m
[32m+[m[32m   ][m
[32m+[m[32m  },[m
   {[m
    "cell_type": "code",[m
    "execution_count": 3,[m
[36m@@ -100,13 +105,13 @@[m
      "name": "stderr",[m
      "output_type": "stream",[m
      "text": [[m
[31m-      "Found cached dataset anli (/Users/sofiamurillosanchez/.cache/huggingface/datasets/anli/plain_text/0.1.0/aabce88453b06dff21c201855ea83283bab0390bff746deadb30b65695755c0b)\n"[m
[32m+[m[32m      "Found cached dataset anli (C:/Users/Zachary/.cache/huggingface/datasets/anli/plain_text/0.1.0/aabce88453b06dff21c201855ea83283bab0390bff746deadb30b65695755c0b)\n"[m
      ][m
     },[m
     {[m
      "data": {[m
       "application/vnd.jupyter.widget-view+json": {[m
[31m-       "model_id": "9473f9a3d7a448999136636d60a54a48",[m
[32m+[m[32m       "model_id": "3ceb7fa9904e471cb4529c81adfc470a",[m
        "version_major": 2,[m
        "version_minor": 0[m
       },[m
[36m@@ -287,40 +292,25 @@[m
   },[m
   {[m
    "cell_type": "code",[m
[31m-   "execution_count": 10,[m
[32m+[m[32m   "execution_count": 8,[m
    "metadata": {},[m
    "outputs": [[m
[31m-    {[m
[31m-     "name": "stderr",[m
[31m-     "output_type": "stream",[m
[31m-     "text": [[m
[31m-      "Exception in thread \"main\" java.lang.NoSuchMethodError: 'java.lang.Object scala.Some.value()'\n",[m
[31m-      "\tat org.apache.spark.internal.Logging.initializeLogging(Logging.scala:138)\n",[m
[31m-      "\tat org.apache.spark.internal.Logging.initializeLogIfNecessary(Logging.scala:115)\n",[m
[31m-      "\tat org.apache.spark.internal.Logging.initializeLogIfNecessary$(Logging.scala:109)\n",[m
[31m-      "\tat org.apache.spark.deploy.SparkSubmit.initializeLogIfNecessary(SparkSubmit.scala:75)\n",[m
[31m-      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:83)\n",[m
[31m-      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046)\n",[m
[31m-      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055)\n",[m
[31m-      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n"[m
[31m-     ][m
[31m-    },[m
     {[m
      "ename": "RuntimeError",[m
      "evalue": "Java gateway process exited before sending its port number",[m
      "output_type": "error",[m
      "traceback": [[m
[31m-      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",[m
[31m-      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",[m
[31m-      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43msparknlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",[m
[31m-      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sparknlp/__init__.py:284\u001b[0m, in \u001b[0;36mstart\u001b[0;34m(gpu, apple_silicon, aarch64, memory, cache_folder, log_folder, cluster_tmp_dir, params, real_time_output, output_level)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkRealTimeOutput()\u001b[38;5;241m.\u001b[39mspark_session\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     spark_session \u001b[38;5;241m=\u001b[39m \u001b[43mstart_without_realtime_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m spark_session\n",[m
[31m-      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sparknlp/__init__.py:182\u001b[0m, in \u001b[0;36mstart.<locals>.start_without_realtime_output\u001b[0;34m()\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    180\u001b[0m         builder\u001b[38;5;241m.\u001b[39mconfig(key, value)\n\u001b[0;32m--> 182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",[m
[31m-      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/session.py:269\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    267\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 269\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[1;32m    272\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",[m
[31m-      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py:483\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 483\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    485\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",[m
[31m-      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py:195\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    193\u001b[0m     )\n\u001b[0;32m--> 195\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[1;32m    198\u001b[0m         master,\n\u001b[1;32m    199\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m         udf_profiler_cls,\n\u001b[1;32m    209\u001b[0m     )\n",[m
[31m-      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py:417\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[0;32m--> 417\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    418\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[1;32m    420\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",[m
[31m-      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/java_gateway.py:106\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[0;32m--> 106\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJava gateway process exited before sending its port number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[1;32m    109\u001b[0m     gateway_port \u001b[38;5;241m=\u001b[39m read_int(info)\n",[m
[31m-      "\u001b[0;31mRuntimeError\u001b[0m: Java gateway process exited before sending its port number"[m
[32m+[m[32m      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",[m
[32m+[m[32m      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",[m
[32m+[m[32m      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43msparknlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",[m
[32m+[m[32m      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sparknlp\\__init__.py:284\u001b[0m, in \u001b[0;36mstart\u001b[1;34m(gpu, apple_silicon, aarch64, memory, cache_folder, log_folder, cluster_tmp_dir, params, real_time_output, output_level)\u001b[0m\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkRealTimeOutput()\u001b[38;5;241m.\u001b[39mspark_session\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 284\u001b[0m     spark_session \u001b[38;5;241m=\u001b[39m \u001b[43mstart_without_realtime_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m spark_session\n",[m
[32m+[m[32m      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sparknlp\\__init__.py:182\u001b[0m, in \u001b[0;36mstart.<locals>.start_without_realtime_output\u001b[1;34m()\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    180\u001b[0m         builder\u001b[38;5;241m.\u001b[39mconfig(key, value)\n\u001b[1;32m--> 182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",[m
[32m+[m[32m      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py:269\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    267\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[0;32m    268\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 269\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[0;32m    272\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",[m
[32m+[m[32m      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\context.py:483\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    481\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    482\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 483\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    484\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    485\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",[m
[32m+[m[32m      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\context.py:195\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    191\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    192\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    193\u001b[0m     )\n\u001b[1;32m--> 195\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[0;32m    198\u001b[0m         master,\n\u001b[0;32m    199\u001b[0m         appName,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    208\u001b[0m         udf_profiler_cls,\n\u001b[0;32m    209\u001b[0m     )\n",[m
[32m+[m[32m      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\context.py:417\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    416\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[1;32m--> 417\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    418\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[0;32m    420\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",[m
[32m+[m[32m      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\java_gateway.py:106\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[1;32m--> 106\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJava gateway process exited before sending its port number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[0;32m    109\u001b[0m     gateway_port \u001b[38;5;241m=\u001b[39m read_int(info)\n",[m
[32m+[m[32m      "\u001b[1;31mRuntimeError\u001b[0m: Java gateway process exited before sending its port number"[m
      ][m
     }[m
    ],[m
[36m@@ -519,7 +509,7 @@[m
    "name": "python",[m
    "nbconvert_exporter": "python",[m
    "pygments_lexer": "ipython3",[m
[31m-   "version": "3.9.7"[m
[32m+[m[32m   "version": "3.9.12"[m
   }[m
  },[m
  "nbformat": 4,[m
[1mdiff --git a/Notebook 2.ipynb b/Notebook 2.ipynb[m
[1mindex 22e6a55..041a4ef 100644[m
[1m--- a/Notebook 2.ipynb[m	
[1m+++ b/Notebook 2.ipynb[m	
[36m@@ -39,18 +39,14 @@[m
   },[m
   {[m
    "cell_type": "code",[m
[31m-   "execution_count": 2,[m
[32m+[m[32m   "execution_count": 3,[m
    "metadata": {},[m
    "outputs": [[m
     {[m
      "name": "stdout",[m
      "output_type": "stream",[m
      "text": [[m
[31m-      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",[m
[31m-      "johnsnowlabs 4.4.4 requires pyspark==3.1.2, but you have pyspark 3.3.0 which is incompatible.\n",[m
[31m-      "johnsnowlabs 4.4.4 requires spark-nlp==4.4.1, but you have spark-nlp 4.3.2 which is incompatible.\n",[m
[31m-      "flair 0.11.1 requires more-itertools~=8.8.0, but you have more-itertools 8.12.0 which is incompatible.\u001b[0m\u001b[31m\n",[m
[31m-      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"[m
[32m+[m[32m      "Note: you may need to restart the kernel to use updated packages.\n"[m
      ][m
     }[m
    ],[m
[36m@@ -60,7 +56,25 @@[m
   },[m
   {[m
    "cell_type": "code",[m
[31m-   "execution_count": 3,[m
[32m+[m[32m   "execution_count": 6,[m
[32m+[m[32m   "metadata": {},[m
[32m+[m[32m   "outputs": [[m
[32m+[m[32m    {[m
[32m+[m[32m     "ename": "SyntaxError",[m
[32m+[m[32m     "evalue": "invalid syntax (1469564055.py, line 1)",[m
[32m+[m[32m     "output_type": "error",[m
[32m+[m[32m     "traceback": [[m
[32m+[m[32m      "\u001b[1;36m  Input \u001b[1;32mIn [6]\u001b[1;36m\u001b[0m\n\u001b[1;33m    nano ~/.bashrc\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"[m
[32m+[m[32m     ][m
[32m+[m[32m    }[m
[32m+[m[32m   ],[m
[32m+[m[32m   "source": [[m
[32m+[m[32m    "nano ~/.bashrc"[m
[32m+[m[32m   ][m
[32m+[m[32m  },[m
[32m+[m[32m  {[m
[32m+[m[32m   "cell_type": "code",[m
[32m+[m[32m   "execution_count": null,[m
    "metadata": {},[m
    "outputs": [],[m
    "source": [[m
[36m@@ -85,7 +99,7 @@[m
   },[m
   {[m
    "cell_type": "code",[m
[31m-   "execution_count": 5,[m
[32m+[m[32m   "execution_count": null,[m
    "metadata": {[m
     "cell_id": "a6d7a0ecc59e4219a74a8f4a9c2c437a",[m
     "deepnote_cell_type": "code",[m
[36m@@ -95,29 +109,7 @@[m
     "scrolled": true,[m
     "source_hash": "b457949f"[m
    },[m
[31m-   "outputs": [[m
[31m-    {[m
[31m-     "name": "stderr",[m
[31m-     "output_type": "stream",[m
[31m-     "text": [[m
[31m-      "Found cached dataset anli (/Users/sofiamurillosanchez/.cache/huggingface/datasets/anli/plain_text/0.1.0/aabce88453b06dff21c201855ea83283bab0390bff746deadb30b65695755c0b)\n"[m
[31m-     ][m
[31m-    },[m
[31m-    {[m
[31m-     "data": {[m
[31m-      "application/vnd.jupyter.widget-view+json": {[m
[31m-       "model_id": "9473f9a3d7a448999136636d60a54a48",[m
[31m-       "version_major": 2,[m
[31m-       "version_minor": 0[m
[31m-      },[m
[31m-      "text/plain": [[m
[31m-       "  0%|          | 0/9 [00:00<?, ?it/s]"[m
[31m-      ][m
[31m-     },[m
[31m-     "metadata": {},[m
[31m-     "output_type": "display_data"[m
[31m-    }[m
[31m-   ],[m
[32m+[m[32m   "outputs": [],[m
    "source": [[m
     "# load datasets and convert them into dataframes\n",[m
     "dataset = load_dataset('anli')\n",[m
[36m@@ -134,7 +126,7 @@[m
   },[m
   {[m
    "cell_type": "code",[m
[31m-   "execution_count": 6,[m
[32m+[m[32m   "execution_count": null,[m
    "metadata": {[m
     "cell_id": "b1f5db0cb46e4ac79b5ed926ce1dbae6",[m
     "deepnote_cell_type": "code",[m
[36m@@ -152,7 +144,7 @@[m
   },[m
   {[m
    "cell_type": "code",[m
[31m-   "execution_count": 7,[m
[32m+[m[32m   "execution_count": null,[m
    "metadata": {[m
     "cell_id": "02e2854c0d9449e0a19a891c0520baee",[m
     "deepnote_cell_type": "code",[m
[36m@@ -161,115 +153,7 @@[m
     "execution_start": 1682702784486,[m
     "source_hash": "5cb1761e"[m
    },[m
[31m-   "outputs": [[m
[31m-    {[m
[31m-     "data": {[m
[31m-      "text/html": [[m
[31m-       "<div>\n",[m
[31m-       "<style scoped>\n",[m
[31m-       "    .dataframe tbody tr th:only-of-type {\n",[m
[31m-       "        vertical-align: middle;\n",[m
[31m-       "    }\n",[m
[31m-       "\n",[m
[31m-       "    .dataframe tbody tr th {\n",[m
[31m-       "        vertical-align: top;\n",[m
[31m-       "    }\n",[m
[31m-       "\n",[m
[31m-       "    .dataframe thead th {\n",[m
[31m-       "        text-align: right;\n",[m
[31m-       "    }\n",[m
[31m-       "</style>\n",[m
[31m-       "<table border=\"1\" class=\"dataframe\">\n",[m
[31m-       "  <thead>\n",[m
[31m-       "    <tr style=\"text-align: right;\">\n",[m
[31m-       "      <th></th>\n",[m
[31m-       "      <th>label</th>\n",[m
[31m-       "      <th>text</th>\n",[m
[31m-       "    </tr>\n",[m
[31m-       "  </thead>\n",[m
[31m-       "  <tbody>\n",[m
[31m-       "    <tr>\n",[m
[31m-       "      <th>0</th>\n",[m
[31m-       "      <td>0</td>\n",[m
[31m-       "      <td>The trolleybus system has over 2 urban routesT...</td>\n",[m
[31m-       "    </tr>\n",[m
[31m-       "    <tr>\n",[m
[31m-       "      <th>1</th>\n",[m
[31m-       "      <td>1</td>\n",[m
[31m-       "      <td>Sharron Macready was a popular character throu...</td>\n",[m
[31m-       "    </tr>\n",[m
[31m-       "    <tr>\n",[m
[31m-       "      <th>2</th>\n",[m
[31m-       "      <td>1</td>\n",[m
[31m-       "      <td>Bastedo didn't keep any pets because of her vi...</td>\n",[m
[31m-       "    </tr>\n",[m
[31m-       "    <tr>\n",[m
[31m-       "      <th>3</th>\n",[m
[31m-       "      <td>1</td>\n",[m
[31m-       "      <td>Alexandra Bastedo was named by her mother.Alex...</td>\n",[m
[31m-       "    </tr>\n",[m
[31m-       "    <tr>\n",[m
[31m-       "      <th>4</th>\n",[m
[31m-       "      <td>1</td>\n",[m
[31m-       "      <td>Bastedo cared for all the animals that inhabit...</td>\n",[m
[31m-       "    </tr>\n",[m
[31m-       "    <tr>\n",[m
[31m-       "      <th>...</th>\n",[m
[31m-       "      <td>...</td>\n",[m
[31m-       "      <td>...</td>\n",[m
[31m-       "    </tr>\n",[m
[31m-       "    <tr>\n",[m
[31m-       "      <th>16941</th>\n",[m
[31m-       "      <td>2</td>\n",[m
[31m-       "      <td>The KNVB cup trophy was awarded to Heerenveen....</td>\n",[m
[31m-       "    </tr>\n",[m
[31m-       "    <tr>\n",[m
[31m-       "      <th>16942</th>\n",[m
[31m-       "      <td>2</td>\n",[m
[31m-       "      <td>20th Century Fox Television is the syndication...</td>\n",[m
[31m-       "    </tr>\n",[m
[31m-       "    <tr>\n",[m
[31m-       "      <th>16943</th>\n",[m
[31m-       "      <td>2</td>\n",[m
[31m-       "      <td>Olivier Rochus always plays with a partner.Oli...</td>\n",[m
[31m-       "    </tr>\n",[m
[31m-       "    <tr>\n",[m
[31m-       "      <th>16944</th>\n",[m
[31m-       "      <td>2</td>\n",[m
[31m-       "      <td>Liisi Rist (born 26 June 1991) is an Estonian ...</td>\n",[m
[31m-       "    </tr>\n",[m
[31m-       "    <tr>\n",[m
[31m-       "      <th>16945</th>\n",[m
[31m-       "      <td>2</td>\n",[m
[31m-       "      <td>3 Mills Studios was established in 1981.The si...</td>\n",[m
[31m-       "    </tr>\n",[m
[31m-       "  </tbody>\n",[m
[31m-       "</table>\n",[m
[31m-       "<p>16946 rows × 2 columns</p>\n",[m
[31m-       "</div>"[m
[31m-      ],[m
[31m-      "text/plain": [[m
[31m-       "       label                                               text\n",[m
[31m-       "0          0  The trolleybus system has over 2 urban routesT...\n",[m
[31m-       "1          1  Sharron Macready was a popular character throu...\n",[m
[31m-       "2          1  Bastedo didn't keep any pets because of her vi...\n",[m
[31m-       "3          1  Alexandra Bastedo was named by her mother.Alex...\n",[m
[31m-       "4          1  Bastedo cared for all the animals that inhabit...\n",[m
[31m-       "...      ...                                                ...\n",[m
[31m-       "16941      2  The KNVB cup trophy was awarded to Heerenveen....\n",[m
[31m-       "16942      2  20th Century Fox Television is the syndication...\n",[m
[31m-       "16943      2  Olivier Rochus always plays with a partner.Oli...\n",[m
[31m-       "16944      2  Liisi Rist (born 26 June 1991) is an Estonian ...\n",[m
[31m-       "16945      2  3 Mills Studios was established in 1981.The si...\n",[m
[31m-       "\n",[m
[31m-       "[16946 rows x 2 columns]"[m
[31m-      ][m
[31m-     },[m
[31m-     "execution_count": 7,[m
[31m-     "metadata": {},[m
[31m-     "output_type": "execute_result"[m
[31m-    }[m
[31m-   ],[m
[32m+[m[32m   "outputs": [],[m
    "source": [[m
     "glove_train_r1 = convert_to_glove(train_r1)\n",[m
     "glove_dev_r1 = convert_to_glove(dev_r1)\n",[m
[36m@@ -287,40 +171,25 @@[m
   },[m
   {[m
    "cell_type": "code",[m
[31m-   "execution_count": 10,[m
[32m+[m[32m   "execution_count": 5,[m
    "metadata": {},[m
    "outputs": [[m
[31m-    {[m
[31m-     "name": "stderr",[m
[31m-     "output_type": "stream",[m
[31m-     "text": [[m
[31m-      "Exception in thread \"main\" java.lang.NoSuchMethodError: 'java.lang.Object scala.Some.value()'\n",[m
[31m-      "\tat org.apache.spark.internal.Logging.initializeLogging(Logging.scala:138)\n",[m
[31m-      "\tat org.apache.spark.internal.Logging.initializeLogIfNecessary(Logging.scala:115)\n",[m
[31m-      "\tat org.apache.spark.internal.Logging.initializeLogIfNecessary$(Logging.scala:109)\n",[m
[31m-      "\tat org.apache.spark.deploy.SparkSubmit.initializeLogIfNecessary(SparkSubmit.scala:75)\n",[m
[31m-      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:83)\n",[m
[31m-      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046)\n",[m
[31m-      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055)\n",[m
[31m-      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n"[m
[31m-     ][m
[31m-    },[m
     {[m
      "ename": "RuntimeError",[m
      "evalue": "Java gateway process exited before sending its port number",[m
      "output_type": "error",[m
      "traceback": [[m
[31m-      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",[m
[31m-      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",[m
[31m-      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43msparknlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",[m
[31m-      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sparknlp/__init__.py:284\u001b[0m, in \u001b[0;36mstart\u001b[0;34m(gpu, apple_silicon, aarch64, memory, cache_folder, log_folder, cluster_tmp_dir, params, real_time_output, output_level)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkRealTimeOutput()\u001b[38;5;241m.\u001b[39mspark_session\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     spark_session \u001b[38;5;241m=\u001b[39m \u001b[43mstart_without_realtime_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m spark_session\n",[m
[31m-      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sparknlp/__init__.py:182\u001b[0m, in \u001b[0;36mstart.<locals>.start_without_realtime_output\u001b[0;34m()\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    180\u001b[0m         builder\u001b[38;5;241m.\u001b[39mconfig(key, value)\n\u001b[0;32m--> 182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",[m
[31m-      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/session.py:269\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    267\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 269\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[1;32m    272\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",[m
[31m-      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py:483\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 483\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    485\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",[m
[31m-      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py:195\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    193\u001b[0m     )\n\u001b[0;32m--> 195\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[1;32m    198\u001b[0m         master,\n\u001b[1;32m    199\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m         udf_profiler_cls,\n\u001b[1;32m    209\u001b[0m     )\n",[m
[31m-      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py:417\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[0;32m--> 417\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    418\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[1;32m    420\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",[m
[31m-      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/java_gateway.py:106\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[0;32m--> 106\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJava gateway process exited before sending its port number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[1;32m    109\u001b[0m     gateway_port \u001b[38;5;241m=\u001b[39m read_int(info)\n",[m
[31m-      "\u001b[0;31mRuntimeError\u001b[0m: Java gateway process exited before sending its port number"[m
[32m+[m[32m      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",[m
[32m+[m[32m      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",[m
[32m+[m[32m      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43msparknlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",[m
[32m+[m[32m      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sparknlp\\__init__.py:284\u001b[0m, in \u001b[0;36mstart\u001b[1;34m(gpu, apple_silicon, aarch64, memory, cache_folder, log_folder, cluster_tmp_dir, params, real_time_output, output_level)\u001b[0m\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkRealTimeOutput()\u001b[38;5;241m.\u001b[39mspark_session\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 284\u001b[0m     spark_session \u001b[38;5;241m=\u001b[39m \u001b[43mstart_without_realtime_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m spark_session\n",[m
[32m+[m[32m      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sparknlp\\__init__.py:182\u001b[0m, in \u001b[0;36mstart.<locals>.start_without_realtime_output\u001b[1;34m()\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    180\u001b[0m         builder\u001b[38;5;241m.\u001b[39mconfig(key, value)\n\u001b[1;32m--> 182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",[m
[32m+[m[32m      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py:269\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    267\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[0;32m    268\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 269\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[0;32m    272\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",[m
[32m+[m[32m      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\context.py:483\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    481\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    482\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 483\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    484\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    485\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",[m
[32m+[m[32m      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\context.py:195\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    191\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    192\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    193\u001b[0m     )\n\u001b[1;32m--> 195\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[0;32m    198\u001b[0m         master,\n\u001b[0;32m    199\u001b[0m         appName,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    208\u001b[0m         udf_profiler_cls,\n\u001b[0;32m    209\u001b[0m     )\n",[m
[32m+[m[32m      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\context.py:417\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    416\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[1;32m--> 417\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    418\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[0;32m    420\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",[m
[32m+[m[32m      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\java_gateway.py:106\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[1;32m--> 106\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJava gateway process exited before sending its port number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[0;32m    109\u001b[0m     gateway_port \u001b[38;5;241m=\u001b[39m read_int(info)\n",[m
[32m+[m[32m      "\u001b[1;31mRuntimeError\u001b[0m: Java gateway process exited before sending its port number"[m
      ][m
     }[m
    ],[m
[36m@@ -519,7 +388,7 @@[m
    "name": "python",[m
    "nbconvert_exporter": "python",[m
    "pygments_lexer": "ipython3",[m
[31m-   "version": "3.9.7"[m
[32m+[m[32m   "version": "3.9.12"[m
   }[m
  },[m
  "nbformat": 4,[m
